{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzD34Gjrnw023FyJ6CSmeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhoaw/ML-Supervised-Prediction-of-Driving-Conditions/blob/main/metodi_prova_INT8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu1Ba0s6ipjl"
      },
      "outputs": [],
      "source": [
        "def quantize_tensor(x, num_bits=8, symmetric=True):\n",
        "    \"\"\"\n",
        "    Quantizza un tensore float32 in int8.\n",
        "    Per la quantizzazione simmetrica, l'intervallo di int8 Ã¨ [-128, 127]\n",
        "    \"\"\"\n",
        "    if symmetric:\n",
        "        qmin, qmax = -128, 127\n",
        "        max_val = max(abs(x.min()), abs(x.max()))\n",
        "        scale = max_val / float(qmax) if max_val != 0 else 1.0\n",
        "        zero_point = 0\n",
        "    else:\n",
        "        qmin, qmax = 0, 255\n",
        "        rmin, rmax = x.min(), x.max()\n",
        "        scale = (rmax - rmin) / (qmax - qmin) if (rmax - rmin) != 0 else 1.0\n",
        "        zero_point = int(np.round(qmin - rmin / scale))\n",
        "    q_x = np.round(x / scale).astype(np.int32) + zero_point\n",
        "    q_x = np.clip(q_x, qmin, qmax).astype(np.int8)\n",
        "    return q_x, scale, zero_point\n",
        "\n",
        "def dequantize_tensor(q_x, scale, zero_point):\n",
        "    \"\"\" Riporta un tensore quantizzato al formato float32 \"\"\"\n",
        "    return scale * (q_x.astype(np.int32) - zero_point)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x, leak = 0):                      # ReLU\n",
        "    return np.where(x <= 0, leak * x, x)\n",
        "\n",
        "def relu_dash(x, leak = 0):                 # ReLU derivative\n",
        "    return np.where(x <= 0, leak, 1)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "    #return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "    return (1 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def softmax(x):\n",
        "  #print(\"x.shape \", x.shape)\n",
        "  # Numerically stable with large exponentials\n",
        "  exps = np.exp(x - np.max(x, axis=0, keepdims=True)) #x.max())\n",
        "  return exps / np.sum(exps, axis=0)\n",
        "\n",
        "def d_softmax(x):\n",
        "  exps = np.exp(x - np.max(x, axis=0, keepdims=True)) #x.max())\n",
        "  return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "\n",
        "\n",
        "def categorical_crossentropy(output, target, from_logits=False):\n",
        "    # Log loss of the correct class of each of our samples\n",
        "  #correct_logprobs = -np.log(probs[np.arange(n_samples), y])\n",
        "  #10x100\n",
        "  # target: batchXnum_class\n",
        "  # output: num_classXbatch\n",
        "   # print(\"---- sum: output.shape \", output.shape, \" target.shape \", target.shape)\n",
        "   # print(\"for target.shape[0] \", target.shape[0])\n",
        "   # print(\"for target.shape[1] \", target.shape[1])\n",
        "    #print(\"target_test \", target.shape) # [40, 10]\n",
        "    #print(\"output \", output.shape) # [10, 40]\n",
        "    vett = []\n",
        "    if from_logits:\n",
        "        output = softmax(output)\n",
        "    else:\n",
        "      for i in range (0,target.shape[0]): #batch\n",
        "        #output /= output.sum(axis=0, keepdims=True)\n",
        "        #output = np.clip(output, 1e-7, 1 - 1e-7)\n",
        "        for j in range (0,target.shape[1]):\n",
        "          if target[i,j] == 1:\n",
        "            vett.append(-np.log(output[j,i]))\n",
        "            break\n",
        "        #vett.append(np.sum(target * -np.log(output), axis=-1, keepdims=False))\n",
        "    return vett"
      ],
      "metadata": {
        "id": "HYgFc2wpitHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    activationFns = {\n",
        "           \"sigmoid\": (sigmoid, d_sigmoid),\n",
        "           \"softmax\": (softmax, d_softmax),\n",
        "           \"relu\": (relu, relu_dash)\n",
        "        }\n",
        "\n",
        "    def __init__(self, inputs, neurons, activation, W_saved, b_saved, retrain): #\n",
        "      if retrain == 0:\n",
        "        np.random.seed(seed=42)\n",
        "        W = (np.random.randn(neurons, inputs)*np.sqrt(1/neurons)).astype(np.float32)\n",
        "        b = np.zeros((neurons, 1))\n",
        "        self.act, self.d_act= self.activationFns.get(activation)\n",
        "      if retrain == 1:\n",
        "        W = W_saved\n",
        "        b = b_saved\n",
        "        self.act, self.d_act= self.activationFns.get(activation)\n",
        "\n",
        "      self.qW, self.scaleW, _ = quantize_tensor(W, symmetric=True)\n",
        "      self.qb, self.scaleb, _ = quantize_tensor(b, symmetric=True)\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, W, b, layer):\n",
        "      if layer != 0:\n",
        "        self.qW = W\n",
        "      self.qb = b\n",
        "\n",
        "    def feedforward(self, A1, layer):\n",
        "      print(\"------------------- feedforward\")\n",
        "      self.A1 = A1\n",
        "      qA1, scale_A1, _ = quantize_tensor(self.A1, symmetric=True); print(\"self.A1.shape \", self.A1.shape)\n",
        "\n",
        "      Z = np.dot(self.qW.astype(np.int32), qA1.astype(np.int32)); #print(\"type(Z) \", type(self.Z))\n",
        "      #z1 = np.dot(qx.astype(np.int32), self.qW1.astype(np.int32)) + self.qb1.astype(np.int32)\n",
        "      Z = Z + np.array(self.qb).astype(np.int32) #.reshape(10, 1)\n",
        "      self.Z = Z * (scale_A1 * self.scaleW); print(\"Z.shape \", self.Z.shape);\n",
        "      self.A = self.act(self.Z); print(\"self.A.shape \", self.A.shape)\n",
        "      return self.A\n",
        "\n",
        "    def backprop(self, dA, learning_rate, layer, target, lambd, num_lyr):\n",
        "      print()\n",
        "      print()\n",
        "      print(\"------------------- backpropagation\")\n",
        "      if layer == num_lyr:\n",
        "        print(\"last layer\")\n",
        "        '''\n",
        "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
        "        change_w['W3'] = np.outer(error, params['A2'])\n",
        "        '''\n",
        "        #dZ = (dA-target.T);\n",
        "        dZ_int32 = dA.astype(np.int32) - (target.T).astype(np.int32);\n",
        "        print(\"dA.shape \", dA.shape); print(\"dZ.shape \", dZ_int32.shape); print(\"target.T[NUM_CLASS][BATCH_SIZE]: \", target.T.shape)\n",
        "        dZ = np.clip(dZ_int32, -128, 127).astype(np.int8)\n",
        "      else:\n",
        "        print(\"layer 1 or 0\")\n",
        "        print(\"dA.shape \", dA.shape)\n",
        "        #dZ = np.multiply(self.d_act(self.Z), dA) #relu #sigmoid\n",
        "        d_act_float = self.d_act(self.Z);\n",
        "        d_act_q, scale_dact, zp_dact = quantize_tensor(d_act_float, symmetric=True)\n",
        "        dZ_int32 = d_act_q.astype(np.int32) * dA.astype(np.int32)\n",
        "        dZ = np.clip(dZ_int32, -128, 127).astype(np.int8)\n",
        "\n",
        "      #dW = (1.0/dZ.shape[1]) * np.matmul(dZ, np.transpose(self.A1));\n",
        "      #dW += (lambd/dZ.shape[1])*self.W\n",
        "      dW_int32 = (1.0/dZ.shape[1]) * np.matmul(dZ.astype(np.int32), (self.A1.T).astype(np.int32))\n",
        "      dW_int32 += (lambd/dZ.shape[1]) * self.qW.astype(np.int32)\n",
        "      dW_float = dW_int32.astype(np.float32)\n",
        "      dW_q, scale_dW, zp_dW = quantize_tensor(dW_float, symmetric=True)\n",
        "      print(\"dW.shape \", dW_q.shape); print(\"np.transpose(self.A1).shape \", np.transpose(self.A1).shape)\n",
        "\n",
        "      #db = (1.0/dZ.shape[1]) * np.sum(dZ, axis=1, keepdims=True)\n",
        "      db_int32 = (1.0/dZ.shape[1]) * np.sum(dZ.astype(np.int32), axis=1, keepdims=True)\n",
        "      db_float = db_int32.astype(np.float32)\n",
        "      db_q, scale_db, zp_db = quantize_tensor(db_float, symmetric=True)\n",
        "\n",
        "      #dA1 = np.dot(self.W.T, dZ); print(\"self.W.T \", self.W.T.shape); print(\"dZ \", dZ.shape)\n",
        "      dA1_int32 = np.dot(self.qW.T.astype(np.int32), dZ.astype(np.int32))\n",
        "      dA1_float = dA1_int32.astype(np.float32)\n",
        "      dA1_q, scale_dA1, zp_dA1 = quantize_tensor(dA1_float, symmetric=True)\n",
        "\n",
        "      #self.W = (self.W - learning_rate * dW)\n",
        "      #self.b = (self.b - learning_rate * db)\n",
        "      W_float = self.qW.astype(np.float32)\n",
        "      b_float = self.qb.astype(np.float32)\n",
        "\n",
        "      W_updated = W_float - learning_rate * dW_q.astype(np.float32)\n",
        "      b_updated = b_float - learning_rate * db_q.astype(np.float32)\n",
        "\n",
        "      self.qW, self.scale_W, self.zp_W = quantize_tensor(W_updated, symmetric=True)\n",
        "      self.qb, self.scale_b, self.zp_b = quantize_tensor(b_updated, symmetric=True)\n",
        "      print(\"self.qW.shape \", self.qW.shape); print(\"type(qW) \", type(self.qW));\n",
        "\n",
        "      return dA1_q, self.qW, self.qb\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6i2BBeZ9itEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tp97SQwKiw9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uAurCnXPiw5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFNtDszZiw2w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}